{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c0b2fd0",
   "metadata": {},
   "source": [
    "# 3주차 수업 내용 정리\n",
    "### e-비즈니스학과 201921507 허진아\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# 1. 텍스트 마이닝의 이해\n",
    "\n",
    "## 1.1 텍스트 마이닝이란?\n",
    "\n",
    "- 텍스트로부터 고양질의 정보를 뽑아내는 과정\n",
    "- 패턴, 트렌드를 살펴봄으로써 알아낼 수 있음\n",
    "- 통계적인 패턴 학습 (Statistical pattern learning)\n",
    "  - 대표적으로 회귀 분석\n",
    "- 비정형 데이터를 분석이 가능한 정형 데이터로 전환하는 것\n",
    "\n",
    "- 일정한 길이의 Vector로 변환\n",
    "- 그리고 변환된 Vector에 머신러닝(딥러닝)기법 적용\n",
    "\n",
    "## 1.2 텍스트 마이닝의 이해를 위한 기본 요구 지식\n",
    "\n",
    "- 자연어 처리\n",
    "- 통계학 & 선형 대수 (조건부 확률, 벡터, 선형 결합)\n",
    "- 머신러닝 (나이브 베이즈 드)\n",
    "- 딥러닝 (CNN, RNN 등)\n",
    "\n",
    "## 1.3 텍스트 마이닝 방법\n",
    "\n",
    "- NLP(Natural Language Processing) 기본도구\n",
    "- 머신러닝(딥러닝)\n",
    "\n",
    "## 1.4 텍스트 마이닝 단계\n",
    "\n",
    "- 문장 → 단어 단위로 쪼개기(tokenize), 표준화(normalize) → 여러개의 표준화된 단어로 이루어진 Sequence 생성(리스트)\n",
    "\n",
    "### 1.4.1 순서를 무시하고 Vector 생성 \n",
    "- 나이브 베이즈\n",
    "- Decision Tree, SVM\n",
    "- logistic Regression\n",
    "  \n",
    "### 1.4.2 순서 정보를 포함한 Vector 생성\n",
    "- Decision Tree, SVM\n",
    "- Logistic Regression, MLP\n",
    "  \n",
    "### 1.4.3. 순서 정보를 유지하면서 Word Embedding된 것으로 표현\n",
    "- RNN, Bi-LSTM, Transformer, Bert\n",
    "\n",
    "## 1.5 텍스트 마이닝 적용 분야\n",
    "- 문서 분류 : 감정 분석\n",
    "- 문서 generation\n",
    "- 키워드 추출\n",
    "- topic modeling\n",
    "\n",
    "## 1.6 텍스트 마이닝 도구 - 파이썬\n",
    "- NLTK\n",
    "- Scikit Leran\n",
    "- Gensim\n",
    "- Keras\n",
    "\n",
    "## 1.7 텍스트 마이닝 기본 도구 (NLP를 중심으로)\n",
    "\n",
    "- 목적 : document, sentence 등을 sparse vector로 변환\n",
    "  - 이 둘은 단어들의 시퀀스\n",
    "  - 여러개의 값을 가진 벡터 중 값이 없는 게 많을 경우 sparse라고 부름\n",
    "### 1.7.1 Tokenize\n",
    "- 문서를 문장의 집합으로 그리고 단어의 집합으로\n",
    "- 의미 없는 단어 걸러냄\n",
    "- 영어 vs 한글\n",
    "  - 영어가 한글 보다 비교적 쉽게 가능\n",
    "        \n",
    "### 1.7.2 Text normalization\n",
    "- 동일한 의미의 단어가 다른 형태를 갖는 것을 보완\n",
    "- Stemming (어간 추출)\n",
    "   - 의미가 아닌 규칙에 의한 변환\n",
    " - Lemmatization (표제어 추출)\n",
    "   - 사전을 이용한 단어의 원형을 추출\n",
    "   - 품사를 고려\n",
    "   \n",
    "### 1.7.3 POS-tagging\n",
    "- 토큰화와 정규화 작업을 통해 나누어진 형태소(의미를 가지는 최소단위)에 따라 품사를 결정하여 할당하는 작업\n",
    "    \n",
    "### 1.7.4 Chunking\n",
    "- 명사구, 형용사구, 분사구 등과 같이 주어와 동사가 없는 두 단어 이상의 집합인 구(Phrase)를 의미\n",
    "- 텍스트로 부터 Information Extraction하기 위한 전 단계\n",
    "  - information Extraction의 대표적인 예) Named Entity Recogmition, NER)\n",
    "  \n",
    "### 1.7.5 BOW\n",
    "- Vector Space model\n",
    "- Count vector\n",
    "\n",
    "### 1.7. 6 TFIDF\n",
    "- 단어의 count를 단어가 나타난 문서의 수로 나눠서 자주 등장하지 않는 단어의 weight를 올림\n",
    "\n",
    "## 1.8 텍스트 마이닝의 문제\n",
    "- Curse of Dimensionality\n",
    "  - 차원의 저주 : 각 데이터 간의 거리가 너무 멀게 위치\n",
    "  - 해결 방법 : 더 많은 데이터 사용 or 차원 축소\n",
    "- 단어 빈도의 불균형\n",
    "  - Zipf's law(멱법칙) : 극히 소수의 데이터가 결정적인 영향을 미치게 됨\n",
    "  - 해결 방안 : Feature selection , boolen BOW 사용, log 등의 함수를 이용해 Weight 변경\n",
    "- 단어가 쓰인 순서 정보의 손실\n",
    "  - 시퀀스 정보의 손실\n",
    "  - 해결 방안 : n-gram (부분적 해결, 주로 분석 문제에 유용), Deep Learning (RNN,Attention, Transfromer, BERT)\n",
    "\n",
    "## 1.9 문제의 해결 방안\n",
    "\n",
    "### 1.9.1 Dimenstionality Reduction (차원 축소)\n",
    "- 차원의 저주를 해결하기 위한 노력\n",
    "- Feature selection ; 만개를 1,000개로 축소\n",
    "- Feature extraction\n",
    "  - 주성분 분석 (PCA,데이터의 분산을 최대한 보존하는 새로운 축을 찾아 변화함으로써 차원을 축소)\n",
    "\n",
    "### 1.9.2 Topic Modeling\n",
    "- 활용 사례 ) 드라마 시청률 변화 ↔ 소셜 미디어 토픽 변화\n",
    "- 시청률이 엄청 높아지면, 사람들이 드라마에 대해 평가를 적음 → 그럼 사람들이 드라마 보면서 관심이 생겨 드라마 시청률에 또 영향을 주게 됨\n",
    "  \n",
    "  \n",
    "### 1.9.3 Embedding\n",
    "- 단어에 대한 Vector의 dimension reduction이 목표\n",
    "- 단어의 표현\n",
    "  - one-hot-encoding extremely sparse\n",
    "  - 각 단어를 모든 문서에서 사용된 단어들의 수 길이의 벡터로 표현, 심한 경우 길이 30만의 벡터 중에 하나만 1인 sparse vector가 됨\n",
    "  - 학습 목적 관점에서 단어의 의미를 내포\n",
    "    \n",
    "### 1.9.4 ELMo (Embeddings from language Model)\n",
    "- 사전에 훈련된 언어 모델을 사용하는 방법론\n",
    "- 이전의 대표적인 임베딩 기법인 Word2Vec 등이 동일한 단어가 문맥에 따라 전혀 다른 의미를 지는 것을 반영하지 못하는 것에 비해 이 방법론은 문맥을 반영하기 위해 개발된 기법\n",
    "  \n",
    "### 1.9.5 Deep learning\n",
    "- RNN\n",
    "- LSTM (Long Short Term Memory)\n",
    "  - 직통 통로를 만들어 RNN의 문제를 해결\n",
    "- 합성곱 신경망 (Convolutional neural networks, CNN)\n",
    "   - 원래 이미지 처리를 위해 개발된 신경망으로 현재는 인간의 이밎 인식보다 더 나은 인식 성능을 보이고 있음\n",
    "   - 그러나, 주변 정보를 학습한다는점을 이용하여 텍스트의 문백을 학습하여 문서를 보류하는 연구가 있었으며 그 이후로는 활용 분야가 넓어짐\n",
    "   - 문서 분류\n",
    "     - 텍스트의 경우 1차원 시퀀스로 표현, 1D CNN 모형을 사용\n",
    "     - 단어 시퀀스에 대해 CNN 필터는 1차원으로만 적용되고 이렇게 텍스트의 특징을 추출한 결과를 마찬가지로 분류기에 넣어 판별\n",
    "      \n",
    "### 1.9.6 Attention\n",
    "- 출력에 나온 어떤 단어는 입력에 있는 특정 단어들에 민감한 것에 착안하여 입력의 단어들로부터 출력 단어에 직접 링크를 만듦\n",
    "  \n",
    "### 1.9.7 Transformer (self - attention)\n",
    "- 입력 단어들끼리도 상호 연관성이 있는 것에 착안하여 단어들 간의 attention 입력과 출력 그리고 출력으로의 attention을 추가\n",
    "- RNN이 사라지고 Self-attention이 이를 대신하여 등장\n",
    "  \n",
    "### 1.9.8 BERT (bidirectional Encoder Representations form Transformer)\n",
    "- 양방향 transformer 인코더를 사용\n",
    "- transfer learning에서 feature와 model을 함께 transfer하고 fine tuning을 통해서 적용하는 방식을 선택"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
