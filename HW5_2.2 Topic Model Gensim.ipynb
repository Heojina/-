{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "741a4ab0",
   "metadata": {},
   "source": [
    "# Topic Modeling with Gensim\n",
    "## 201921507 허진아\n",
    "\n",
    "pip install gensim\n",
    "pip install pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4120b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "#메일 내용에서 hint가 되는 부분을 삭제 - 순수하게 내용만으로\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11afce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 library들을 import\n",
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efa39b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    min_length = 3 #최소 단어 크기\n",
    "    # nltk의 tokenizer를 이용해서 word 추출한 후에 소문자로 변환\n",
    "    words = map(lambda word: word.lower(), word_tokenize(text))\n",
    "    #words = word_tokenize(text.lower()) #이렇게 해도 되는지 확인\n",
    "    # stopwords 제외\n",
    "    words = [word for word in words if word not in cachedStopWords]\n",
    "    # portr stemmer 적용\n",
    "    tokens = (list(map(lambda token: PorterStemmer().stem(token),words)))\n",
    "    #tokens = [PorterStemmer().stem(token) for token in words]  #이렇게 해도 되는지 확인\n",
    "    #알파벳으로 이루어진 단어들만 추출\n",
    "    p = re.compile('[a-zA-Z]+');\n",
    "    filtered_tokens = list(filter (lambda token: p.match(token) and len(token) >= min_length,tokens))\n",
    "    #filtered_tokens = [token for token in tokens if p.match(token) and len(token) >= min_length]   #이렇게 해도 되는지 확인\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b302b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [tokenize(doc) for doc in newsgroups_train.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef12c656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "['notic', 'save', 'model', 'map', 'plane', 'posit', 'care', 'file', 'reload', 'restart', 'given', 'default', 'posit', 'orient', 'save', 'file', 'positions/orient', 'preserv', 'anyon', 'know', 'inform', 'store', 'file', 'noth', 'explicitli', 'said', 'manual', 'save', 'textur', 'rule', 'file', 'like', 'abl', 'read', 'textur', 'rule', 'inform', 'anyon', 'format', 'file', 'file', 'format', 'avail', 'somewher', 'rych']\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.data[0])\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee29d78",
   "metadata": {},
   "source": [
    "**Gensim으로 topic modeling을 하기 위해서는 Dictionary, Corpus, Model의 3단계를 거쳐야 함**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e0e4518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in initital documents: 19661\n",
      "Number of unique words after removing rare and common words: 5628\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "print('Number of unique words in initital documents:', len(dictionary))\n",
    "\n",
    "# Filter out words that occur less than 10 documents, or more than 20% of the documents.\n",
    "dictionary.filter_extremes(no_below=3, no_above=0.5)\n",
    "print('Number of unique words after removing rare and common words:', len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15562274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 5628\n",
      "Number of documents: 2034\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce681b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 34s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "# Set training parameters.\n",
    "num_topics = 25\n",
    "chunksize = 500 # size of the doc looked at every pass\n",
    "passes = 20 # number of passes through documents\n",
    "iterations = 40\n",
    "eval_every = 1  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "%time model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                       alpha='auto', eta='auto', \\\n",
    "                       iterations=iterations, num_topics=num_topics, \\\n",
    "                       passes=passes, eval_every=eval_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6b9bd11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis\n",
      "  Using cached pyLDAvis-3.3.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gjwls\\anaconda3 new\\lib\\site-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: future in c:\\users\\gjwls\\anaconda3 new\\lib\\site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\gjwls\\anaconda3 new\\lib\\site-packages (from pyLDAvis) (1.2.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\gjwls\\anaconda3 new\\lib\\site-packages (from pyLDAvis) (0.24.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\gjwls\\anaconda3 new\\lib\\site-packages (from pyLDAvis) (4.1.2)\n",
      "Requirement already satisfied: numexpr in c:\\users\\gjwls\\anaconda3 new\\lib\\site-packages (from pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gjwls\\anaconda3 new\\lib\\site-packages (from pyLDAvis) (52.0.0.post20210125)\n",
      "Collecting funcy\n",
      "  Using cached funcy-1.16-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\gjwls\\anaconda3 new\\lib\\site-packages (from pyLDAvis) (1.21.4)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: scipy in c:\\users\\gjwls\\anaconda3 new\\lib\\site-packages (from pyLDAvis) (1.6.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\gjwls\\anaconda3 new\\lib\\site-packages (from pyLDAvis) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\gjwls\\appdata\\roaming\\python\\python38\\site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\gjwls\\anaconda3 new\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2021.1)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\gjwls\\anaconda3 new\\lib\\site-packages (from gensim->pyLDAvis) (0.29.23)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\gjwls\\anaconda3 new\\lib\\site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\gjwls\\anaconda3 new\\lib\\site-packages (from jinja2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\gjwls\\anaconda3 new\\lib\\site-packages (from scikit-learn->pyLDAvis) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gjwls\\appdata\\roaming\\python\\python38\\site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
      "Installing collected packages: sklearn, funcy, pyLDAvis\n",
      "Successfully installed funcy-1.16 pyLDAvis-3.3.1 sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a898a344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\gjwls\\anaconda3 new\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\gjwls\\anaconda3 new\\lib\\site-packages (from gensim) (1.21.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\gjwls\\anaconda3 new\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\gjwls\\anaconda3 new\\lib\\site-packages (from gensim) (0.29.23)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\gjwls\\anaconda3 new\\lib\\site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ab02240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pyldavis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e48d8861",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyLDAvis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4d1d7b9bde71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensimv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pyLDAvis' is not defined"
     ]
    }
   ],
   "source": [
    "pyLDAvis.gensimv.prepare(model, corpus, dictionary, sort=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
