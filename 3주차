# 1. 텍스트 마이닝의 이해

## 1.1 텍스트 마이닝이란?

- 텍스트로부터 고양질의 정보를 뽑아내는 과정
- 패턴, 트렌드를 살펴봄으로써 알아낼 수 있음
- 통계적인 패턴 학습 (Statistical pattern learning)
    - 대표적으로 회귀 분석
- 비정형 데이터를 분석이 가능한 정형 데이터로 전환하는 것

- 일정한 길이의 Vector로 변환
- 그리고 변환된 Vector에 머신러닝(딥러닝)기법 적용

## 1.2 텍스트 마이닝의 이해를 위한 기본 요구 지식

- 자연어 처리
- 통계학 & 선형 대수 (조건부 확률, 벡터, 선형 결합)
- 머신러닝 (나이브 베이즈 드)
- 딥러닝 (CNN, RNN 등)

## 1.3 텍스트 마이닝 방법

- NLP(Natural Language Processing) 기본도구
- 머신러닝(딥러닝)

## 1.4 텍스트 마이닝 단계

- 문장 → 단어 단위로 쪼개기(tokenize), 표준화(normalize) → 여러개의 표준화된 단어로 이루어진 Sequence 생성(리스트)
1. 순서를 무시하고 Vector 생성 
    1. 나이브 베이즈
    2. Decision Tree, SVM
    3. logistic Regression
2. 순서 정보를 포함한 Vector 생성
    1. Decision Tree, SVM
    2. Logistic Regression, MLP
3. 순서 정보를 유지하면서 Word Embedding된 것으로 표현
    1. RNN, Bi-LSTM, Transformer, Bert

## 1.5 텍스트 마이닝 적용 분야

- 문서 분류 : 감정 분석
- 문서 generation
- 키워드 추출
- topic modeling

## 1.6 텍스트 마이닝 도구 - 파이썬

- NLTK
- Scikit Leran
- Gensim
- Keras

## 1.7 텍스트 마이닝 기본 도구 (NLP를 중심으로)

- 목적 : document, sentence 등을 sparse vector로 변환
    - 이 둘은 단어들의 시퀀스
    - 여러개의 값을 가진 벡터 중 값이 없는 게 많을 경우 sparse라고 부름
- Tokenize
    - 문서를 문장의 집합으로 그리고 단어의 집합으로
    - 의미 없는 단어 걸러냄
    - 영어 vs 한글
        - 영어가 한글 보다 비교적 쉽게 가능
- Text normalization
    - 동일한 의미의 단어가 다른 형태를 갖는 것을 보완
    - Stemming (어간 추출)
        - 의미가 아닌 규칙에 의한 변환
    - Lemmatization (표제어 추출)
        - 사전을 이용한 단어의 원형을 추출
        - 품사를 고려
- POS-tagging
    - 토큰화와 정규화 작업을 통해 나누어진 형태소(의미를 가지는 최소단위)에 따라 품사를 결정하여 할당하는 작업
- Chunking
    - 명사구, 형용사구, 분사구 등과 같이 주어와 동사가 없는 두 단어 이상의 집합인 구(Phrase)를 의미
    - 텍스트로 부터 Information Extraction하기 위한 전 단계
        - information Extraction의 대표적인 예) Named Entity Recogmition, NER)
- BOW
    - Vector Space model
    - Count vector
- TFIDF
    - 단어의 count를 단어가 나타난 문서의 수로 나눠서 자주 등장하지 않는 단어의 weight를 올림

## 1.8 텍스트 마이닝의 문제

- Curse of Dimensionality
    - 차원의 저주 : 각 데이터 간의 거리가 너무 멀게 위치
    - 해결 방법 : 더 많은 데이터 사용 or 차원 축소
- 단어 빈도의 불균형
    - Zipf's law(멱법칙) : 극히 소수의 데이터가 결정적인 영향을 미치게 됨
    - 해결 방안 : Feature selection , boolen BOW 사용, log 등의 함수를 이용해 Weight 변경
- 단어가 쓰인 순서 정보의 손실
    - 시퀀스 정보의 손실
    - 해결 방안 : n-gram (부분적 해결, 주로 분석 문제에 유용), Deep Learning (RNN,Attention, Transfromer, BERT)

## 1.9 문제의 해결 방안

- Dimenstionality Reduction (차원 축소)
    - 차원의 저주를 해결하기 위한 노력
    - Feature selection ; 만개를 1,000개로 축소
    - Feature extraction
        - 주성분 분석 (PCA,데이터의 분산을 최대한 보존하는 새로운 축을 찾아 변화함으로써 차원을 축소)
    - Topic Modeling
        - 활용 사례 ) 드라마 시청률 변화 ↔ 소셜 미디어 토픽 변화
            - 시청률이 엄청 높아지면, 사람들이 드라마에 대해 평가를 적음 → 그럼 사람들이 드라마 보면서 관심이 생겨 드라마 시청률에 또 영향을 주게 됨
    - Embedding
        - 단어에 대한 Vector의 dimension reduction이 목표
        - 단어의 표현
            - one-hot-encoding extremely sparse
                - 각 단어를 모든 문서에서 사용된 단어들의 수 길이의 벡터로 표현, 심한 경우 길이 30만의 벡터 중에 하나만 1인 sparse vector가 됨
        - 학습 목적 관점에서 단어의 의미를 내포
        - ELMo (Embeddings from language Model)
            - 사전에 훈련된 언어 모델을 사용하는 방법론
            - 이전의 대표적인 임베딩 기법인 Word2Vec 등이 동일한 단어가 문맥에 따라 전혀 다른 의미를 지는 것을 반영하지 못하는 것에 비해 이 방법론은 문맥을 반영하기 위해 개발된 기법
    - Deep learning
        - RNN
        - LSTM (Long Short Term Memory)
            - 직통 통로를 만들어 RNN의 문제를 해결
        - 합성곱 신경망 (Convolutional neural networks, CNN)
            - 원래 이미지 처리를 위해 개발된 신경망으로 현재는 인간의 이밎 인식보다 더 나은 인식 성능을 보이고 있음
            - 그러나, 주변 정보를 학습한다는점을 이용하여 텍스트의 문백을 학습하여 문서를 보류하는 연구가 있었으며 그 이후로는 활용 분야가 넓어짐
            - 문서 분류
                - 텍스트의 경우 1차원 시퀀스로 표현, 1D CNN 모형을 사용
                - 단어 시퀀스에 대해 CNN 필터는 1차원으로만 적용되고 이렇게 텍스트의 특징을 추출한 결과를 마찬가지로 분류기에 넣어 판별
    - Attention
        - 출력에 나온 어떤 단어는 입력에 있는 특정 단어들에 민감한 것에 착안하여 입력의 단어들로부터 출력 단어에 직접 링크를 만듦
    - Transformer (self - attention)
        - 입력 단어들끼리도 상호 연관성이 있는 것에 착안하여 단어들 간의 attention 입력과 출력 그리고 출력으로의 attention을 추가
        - RNN이 사라지고 Self-attention이 이를 대신하여 등장
    - BERT (bidirectional Encoder Representations form Transformer)
        - 양방향 transformer 인코더를 사용
        - transfer learning에서 feature와 model을 함께 transfer하고 fine tuning을 통해서 적용하는 방식을 선택
