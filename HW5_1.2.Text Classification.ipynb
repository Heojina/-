{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1874d9e8",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "## 201921507 허진아\n",
    "텍스트 문서의 다양한 분류방법에 대해 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db0be1d",
   "metadata": {},
   "source": [
    "## 데이터 준비 (복습)\n",
    "이전에 사용한 영화리뷰 데이터를 이용해서 주어진 리뷰 내용에 대해 positive와 negative를 분류하는 분류기를 학습하고자 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2492913d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews count: 2000\n",
      "Length of the first review: 4043\n",
      "Labels: {'neg', 'pos'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "fileids = movie_reviews.fileids() #movie review data에서 file id를 가져옴\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in fileids] #file id를 이용해 raw text file을 가져옴\n",
    "categories = [movie_reviews.categories(fileid)[0] for fileid in fileids] \n",
    "#file id를 이용해 label로 사용할 category 즉 positive와 negative 정보를 순서대로 가져옴\n",
    "\n",
    "print('Reviews count:', len(reviews))\n",
    "print('Length of the first review:', len(reviews[0]))\n",
    "print('Labels:', set(categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b35271d",
   "metadata": {},
   "source": [
    "### train set와 test set의 분리\n",
    "\n",
    "적절한 비율로 train set과 test set을 분리하여 저장  \n",
    "train set은 학습에 사용되고, test set은 검증에 사용\n",
    "default로 shuffle을 함: train set과 test set이 고르게 분포되도록 하기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d5a865a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set count:  1600\n",
      "Test set count:  400\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split #sklearn에서 제공하는 split 함수를 사용\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, categories, test_size=0.2, random_state=10)\n",
    "# sklearn의 train_test_split 함수는 먼저 data set을 shuffle하고 주어진 비율에 따라 train set과 test set을 나눠 줌\n",
    "# 위에서는 reviews를 X_train과 X_test로 8:2의 비율로 나누고, categories를 y_train과 y_test로 나눔\n",
    "# 이 때 X와 y의 순서는 동일하게 유지해서 각 입력값과 label이 정확하게 match되도록 함\n",
    "# random_state는 shuffle에서의 seed 값으로, 지정한 경우 항상 동일한 결과로 shuffle이 됨\n",
    "\n",
    "print('Train set count: ', len(X_train))\n",
    "print('Test set count: ', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d57d8af",
   "metadata": {},
   "source": [
    "### TFIDF 변환\n",
    "CountVectorizer로 Count Vector를 생성하고 TFIDF로 변환하는 대신, text로부터 직접 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20cdc6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#sklearn에서 제공하는 TfidfVectorizer를 이용\n",
    "tfidf = TfidfVectorizer().fit(X_train) # X_train을 이용하여 vectorizer를 학습\n",
    "tfidf #vectorize에서 사용한 매개변수 값들을 확인 -> 현재는 모두 default 값을 사용, 향후 tokenizer, max_features 등을 지정할 수 있음\n",
    "# 상세한 매개변수 내용은 위 링크를 참조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fe13ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 36310)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf = tfidf.transform(X_train) #학습된 vectorizer를 이용하여 train set을 변환\n",
    "X_train_tfidf.shape # 1600 (review 수) x 36310 (전체 corpus에서 사용된 단어의 수) 크기로 vector set이 생성됨\n",
    "# matrix 안의 값은 해당 tfidf score임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af22e544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=2000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features=2000).fit(X_train) #사용된 단어의 수가 너무 많은 경우, max_feature를 제한하여 학습이 가능\n",
    "tfidf #max_factures 값이 사용된 것을 볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "791ac325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set dimension: (1600, 2000)\n",
      "Test set dimension: (400, 2000)\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf = tfidf.transform(X_train) # train set을 변환\n",
    "print('Train set dimension:', X_train_tfidf.shape) # 36310 대신 2000이 된 것을 확인\n",
    "X_test_tfidf = tfidf.transform(X_test) # test set을 변환\n",
    "print('Test set dimension:', X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbd368c",
   "metadata": {},
   "source": [
    "# Naive Bayse Classfier (Scikit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1477250c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set dimension: (1600, 2000)\n",
      "Test set dimension: (400, 2000)\n"
     ]
    }
   ],
   "source": [
    "#나이브 베이즈는 word count를 사용하므로 tfdif가 아닌 count vectorizer를 사용하여 학습 및 변환\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features=2000).fit(X_train) #tfidf와 동일하게 max_feature를 제한하여 학습\n",
    "X_train_cv = cv.transform(X_train) # train set을 변환\n",
    "print('Train set dimension:', X_train_cv.shape) # 36310 대신 2000이 된 것을 확인\n",
    "X_test_cv = cv.transform(X_test) # test set을 변환\n",
    "print('Test set dimension:', X_test_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "905a91f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB #sklearn이 제공하는 MultinomialNB 를 사용\n",
    "NB_clf = MultinomialNB() # 분류기 선언\n",
    "\n",
    "NB_clf.fit(X_train_cv, y_train) #train set을 이용하여 분류기(classifier)를 학습\n",
    "#NB_clf.fit(X_train_tfidf, y_train) #tfidf 값을 사용할 수도 있으나, NB 이론에 맞지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91038338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.864\n",
      "Test set score: 0.775\n"
     ]
    }
   ],
   "source": [
    "print('Train set score: {:.3f}'.format(NB_clf.score(X_train_cv, y_train))) #train set에 대한 예측정확도를 확인\n",
    "print('Test set score: {:.3f}'.format(NB_clf.score(X_test_cv, y_test))) #test set에 대한 예측정확도를 확인\n",
    "#실제로 필요한 것은 test set에 대한 예측정확도이나, 과적합 (overfitting)의 문제가 있는지를 보기 위해 train set에 대한 예측정확도를 같이 확인\n",
    "#print('Train set score: {:.3f}'.format(NB_clf.score(X_train_tfidf, y_train)))\n",
    "#print('Test set score: {:.3f}'.format(NB_clf.score(X_test_tfidf, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72889e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos' 'neg' 'pos']\n",
      "['neg']\n"
     ]
    }
   ],
   "source": [
    "#여러 문장에 대해 count vectorier로 변환 후 학습된 분류기로 결과를 예측\n",
    "print(NB_clf.predict(cv.transform(['the story was unimaginative', 'the plot was ludicrous', 'kate winslet is accessible'])))\n",
    "\n",
    "#위 첫째 문장에서 story를 actor로 바꿔서 예측\n",
    "print(NB_clf.predict(cv.transform(['the actor was unimaginative'])))\n",
    "#동일한 형용사라도 대상에 따라 결과가 바뀔 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f486662",
   "metadata": {},
   "source": [
    "# Logistic Regresstion (Scikit)\n",
    "예측하고자 하는 값 혹은 label이 연속적인 값이 아니고 분류(class)일 때 사용하는 regression 방법  \n",
    "분류는 binary인 경우와 multi-class인 경우가 있음  \n",
    "지금은 positive와 negative 두 class 중에서 선택하므로 binary classification 문제임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5eb3af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 1.000\n",
      "Test set score: 0.825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gjwls\\anaconda3 new\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression #sklearn이 제공하는 logistic regression을 사용\n",
    "\n",
    "#count vector에 대해 regression을 해서 NB와 비교\n",
    "LR_clf_cv = LogisticRegression() #분류기 선언\n",
    "LR_clf_cv.fit(X_train_cv, y_train) # train data를 이용하여 분류기를 학습\n",
    "print('Train set score: {:.3f}'.format(LR_clf_cv.score(X_train_cv, y_train))) # train data에 대한 예측정확도 \n",
    "print('Test set score: {:.3f}'.format(LR_clf_cv.score(X_test_cv, y_test))) # test data에 대한 예측정확도\n",
    "# count vector를 이용한 regression 결과가 tfidf보다 더 좋게 나옴\n",
    "# 보통은 tfidf가 더 좋은 결과를 보이는데, 이와 같이 상황에 따라 다른 결과가 나오기도 함\n",
    "# 지금은 train data의 수가 1,600개인데 비해, 추정해야 하는 parameter의 수가 2,000개로 sample 수가 학습에 부족한 상황, \n",
    "# 따라서 예상 못한 다양한 결과가 나올 수 있음\n",
    "# 좀더 data가 많은 상황에서의 test가 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00c15730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.917\n",
      "Test set score: 0.820\n"
     ]
    }
   ],
   "source": [
    "#tfidf vector를 이용해서 분류기 학습\n",
    "LR_clf = LogisticRegression() #분류기 선언\n",
    "LR_clf.fit(X_train_tfidf, y_train) # train data를 이용하여 분류기를 학습\n",
    "print('Train set score: {:.3f}'.format(LR_clf.score(X_train_tfidf, y_train))) # train data에 대한 예측정확도 \n",
    "print('Test set score: {:.3f}'.format(LR_clf.score(X_test_tfidf, y_test))) # test data에 대한 예측정확도\n",
    "# NB에 비해 더 좋은 결과가 나오는 것을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6441f765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neg', 'neg', 'pos'], dtype='<U3')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NB 분류기에서 사용했던 예제로 결과 확인, 실제로 결과가 더 나아졌음을 확인할 수 있음\n",
    "LR_clf.predict(tfidf.transform(['the story was unimaginative', 'the plot was ludicrous', 'kate winslet is accessible']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c270eedb",
   "metadata": {},
   "source": [
    "## Ridge regression\n",
    "\n",
    "**능선 회귀**로 알려진 tikhonov 정규화는 고유한 솔루션이 없는 방정식에 대한 답을 근사하기 위해 가장 일반적으로 사용되는 회귀 알고리즘이다.  \n",
    "이러한 유형의 문제는 제한된 데이터를 사용하여 \"최상의\" 솔루션을 선택해야 하는 기계 학습 작업에서 매우 일반적입니다.\n",
    "\n",
    "잘못된 문제를 **정규화**하는 가장 일반적으로 사용되는 방법입니다.  \n",
    "통계에서는 이 방법을 **능선 회귀**라고 하고 기계 학습에서는 **가중치 감쇠**라고 하며 여러 독립적인 발견을 사용한다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5524ca2f",
   "metadata": {},
   "source": [
    "Ridge regression은 과적합을 방지하기 위해 사용됨\n",
    "과적합은 분류기가 training data에 지나치게 fitting되어 실제 상황 혹은 test data에서는 좋은 성능이 나타나지 않는 상황을 말한다.  \n",
    "\n",
    "위 그림에서 파란색의 선은 보다 간단한 초록색의 선으로 주어진 데이터를 설명할 수 있음에도 불구하고, 훨씬 복잡한 곡선으로 fitting이 되어 있으며, 이로 인해 training data에 대한 예측정확도는 높으나 test data에 대해서는 예측정확도가 떨어진다.  \n",
    "이와 같은 현상을 방지하기 위해서는 곡선이 지나치게 복잡해지지 않도록 parameter를 억제하면 된다.  \n",
    "\n",
    "Rigde regression은 parameter에 대해 제약을 줌으로써 학습과정에서 parameter가 과도하게 변하지 않도록 한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ef1dbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.974\n",
      "Test set score: 0.838\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "ridge_clf = RidgeClassifier() #릿지 분류기 선언\n",
    "ridge_clf.fit(X_train_tfidf, y_train) #학습\n",
    "print('Train set score: {:.3f}'.format(ridge_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score: {:.3f}'.format(ridge_clf.score(X_test_tfidf, y_test)))\n",
    "# 일반적으로 ridge regression을 쓰면 쓰지 않은 경우보다 train data에 대한 예측정확도는 떨어지고 test data는 올라가게 됨\n",
    "# 여기서는 train set에 대한 예측정확도가 같이 상승하는 진귀한 경우가 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7689b45d",
   "metadata": {},
   "source": [
    "## Lasso regression for feature selection\n",
    "\n",
    "통계 및 기계 학습에서 올가미(최소 절대 수축 및 선택 연산자)(또한 Lasso 또는 LASSO)는 생성하는 통계 모델의 예측 정확도와 해석 가능성을 향상시키기 위해 변수 선택과 정규화를 모두 수행하는 회귀 분석 방법이다.  \n",
    "\n",
    "Lasso regression은 ridge regression과 비슷하게 parameter에 제약을 주지만, 0에 가까운 parameter를 완전히 0으로 바꿔서 결과적으로 feature를 선택하게 된다는 차이가 있다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67919f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.804\n",
      "Test set score: 0.770\n",
      "Used features count: 78 out of 2000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "lasso_clf = LogisticRegression(penalty='l1', solver='liblinear') # Lasso는 동일한 LogisticRegression을 사용하면서 매개변수로 지정\n",
    "lasso_clf.fit(X_train_tfidf, y_train) # train data로 학습\n",
    "print('Train set score: {:.3f}'.format(lasso_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score: {:.3f}'.format(lasso_clf.score(X_test_tfidf, y_test)))\n",
    "print('Used features count: {}'.format(np.sum(lasso_clf.coef_ != 0)), 'out of', X_train_tfidf.shape[1]) \n",
    "# parameter 혹은 coefficient 중에서 0이 아닌 것들의 개수를 출력\n",
    "# 2000개 중에서 78개만 선택된 것을 볼 수 있음\n",
    "# 예측률은 rigde나 일반 logistic에 비해 떨어지지만, 실제로 영향을 미치는 단어들이 어떤 것들인지 확인할 수 있다는 장점이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8685b2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['000', '10', '100', '13', '15', '1995', '1996', '1997', '1998', '1999']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(tfidf.vocabulary_)) # tfidf에 사용된 단어의 수\n",
    "tfidf_voca = tfidf.get_feature_names() # tfidf에서 단어이름을 가져옴\n",
    "tfidf_voca[:10] # 앞 10개를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e770acc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "print((lasso_clf.coef_ != 0)[0].tolist()[:100]) \n",
    "# lasso에 사용된 단어들 중 coefficient의 사용여부를 리스트로 변환하여 앞부터 100개를 출력해 봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97e9d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = []\n",
    "for i, sign in enumerate((lasso_clf.coef_ != 0)[0].tolist()):\n",
    "    if sign: selected_features.append(tfidf_voca[i]) #사용여부가 True인 단어들만 selected_features에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a740e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aliens', 'also', 'and', 'any', 'as', 'attempt', 'awful', 'bad', 'batman', 'boring', 'cameron', 'carpenter', 'could', 'definitely', 'director', 'dull', 'even', 'excellent', 'family', 'great', 'hanks', 'harry', 'have', 'he', 'her', 'here', 'his', 'in', 'is', 'jackie', 'julie', 'lame', 'least', 'lebowski', 'life', 'looks', 'many', 'matrix', 'mess', 'most', 'movie', 'mulan', 'no', 'nothing', 'off', 'on', 'only', 'perfect', 'perfectly', 'plot', 'political', 'poor', 'quite', 'reason', 'ridiculous', 'scream', 'script', 'seagal', 'see', 'seen', 'shrek', 'so', 'stupid', 'supposed', 'the', 'then', 'there', 'this', 'to', 'truman', 'unfortunately', 'very', 'war', 'waste', 'well', 'west', 'why', 'worst']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(selected_features) #78개의 선택된 단어들을 출력 - 즉 positive, negative에 결정적 영향을 미치는 단어들\n",
    "len(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f8e9945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aliens'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_voca[(lasso_clf.coef_ != 0)[0].tolist().index(True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0609d3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aliens', 'also', 'and', 'any', 'as', 'attempt', 'awful', 'bad', 'batman', 'boring', 'cameron', 'carpenter', 'could', 'definitely', 'director', 'dull', 'even', 'excellent', 'family', 'great', 'hanks', 'harry', 'have', 'he', 'her', 'here', 'his', 'in', 'is', 'jackie', 'julie', 'lame', 'least', 'lebowski', 'life', 'looks', 'many', 'matrix', 'mess', 'most', 'movie', 'mulan', 'no', 'nothing', 'off', 'on', 'only', 'perfect', 'perfectly', 'plot', 'political', 'poor', 'quite', 'reason', 'ridiculous', 'scream', 'script', 'seagal', 'see', 'seen', 'shrek', 'so', 'stupid', 'supposed', 'the', 'then', 'there', 'this', 'to', 'truman', 'unfortunately', 'very', 'war', 'waste', 'well', 'west', 'why', 'worst']\n"
     ]
    }
   ],
   "source": [
    "print([tfidf_voca[i] for i, j in enumerate((lasso_clf.coef_ != 0)[0].tolist()) if j]) #선택된 단어들을 출력하는 또다른 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5ebc63",
   "metadata": {},
   "source": [
    "## Dimension을 줄이는 또 다른 방법 : SVD\n",
    "lasso가 feature selection이라면, SVD는 feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37984a20",
   "metadata": {},
   "source": [
    "## PCA(Principal component analysis)\n",
    "**주성분 분석(PCA)** \n",
    "1. 직교 변환을 사용하여 상관 가능성이 있는 변수의 관측값 집합을 주성분이라고 하는 선형 비상관 변수 값 집합으로 변환하는 통계 절차d이다.  \n",
    "\n",
    "2. 데이터의 분산을 최대한 보존하는 새로운 축을 찾아 변환함으로써 차원을 축소한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb5a832",
   "metadata": {},
   "source": [
    "## LSA(Latent Semantic Analysis)\n",
    "**Latent semantic analysis(LSA)** 은 자연어 처리 기술, 특히 배포 의미 체계에서 문서 및 용어와 관련된 개념 집합을 생성하여 문서 집합과 문서에 포함된 용어 간의 관계를 분석한다.   \n",
    "LSA는 의미가 가까운 단어가 유사한 텍스트에서 나타날 것이라고 가정한다(분포 가설). 단락당 단어 수를 포함하는 행렬(행은 고유한 단어를 나타내고 열은 각 단락을 나타냄)은 큰 텍스트 조각으로 구성되며  **Singular value decomposition(SVD)** 라는 수학적 기술을 사용하여 유사성 구조를 유지하면서 행 수를 줄인다. 열 중. 그런 다음 두 행에 의해 형성된 두 벡터 사이의 각도(또는 두 벡터의 정규화 사이의 내적)의 코사인을 취하여 단어를 비교합니다. 1에 가까운 값은 매우 유사한 단어를 나타내고 0에 가까운 값은 매우 다른 단어를 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c13c321",
   "metadata": {},
   "source": [
    "**SVD(특이값 분해)** \n",
    "<br>\"*truncated SVD*는 Σ 행렬의 대각원소(특이값) 가운데 상위 t개만 골라낸 형태. 이렇게 하면 행렬 A를 원복할 수 없게 되지만, 데이터 정보를 상당히 압축했음에도 행렬 A를 근사할 수 있음.\" **잠재의미분석(LSA)** 은 바로 이 방법을 사용\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe05989",
   "metadata": {},
   "source": [
    "## Scikit-learn LSA\n",
    "**Dimensionality reduction using truncated SVD (aka LSA)**\n",
    "\n",
    "이 변환기는 절단된 특이값 분해(SVD)를 통해 선형 차원 축소를 수행한다.  \n",
    "PCA와 달리 이 추정기는 특이값 분해를 계산하기 전에 데이터를 중앙에 배치하지 않는다.  \n",
    "이것은 scipy.sparse 행렬을 효율적으로 사용할 수 있음을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77a4471f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01525261 0.01669138 0.0133583  0.01042773 0.00741403 0.00730082\n",
      " 0.00677966 0.00583257 0.00555167 0.00520324 0.00493018 0.00489498\n",
      " 0.00458197 0.0044574  0.00432849 0.00421975 0.0041921  0.00413251\n",
      " 0.00403416 0.00398954 0.00388074 0.00379435 0.00377392 0.00370049\n",
      " 0.00361687 0.00358558 0.00350955 0.00340951 0.0033149  0.00327589\n",
      " 0.00321078 0.00319169 0.00315556 0.00311055 0.00304756 0.00302892\n",
      " 0.00300949 0.00297231 0.00293611 0.00292412 0.00290208 0.00287548\n",
      " 0.0028342  0.00280742 0.00276097 0.00274277 0.00270645 0.00268513\n",
      " 0.00267427 0.00265801 0.00258256 0.00257419 0.00256683 0.00255658\n",
      " 0.00251155 0.00250159 0.00248046 0.00247207 0.00245957 0.00245289\n",
      " 0.00241614 0.0023879  0.00236025 0.00234094 0.00233893 0.00231345\n",
      " 0.00229516 0.0022807  0.00227705 0.00224271 0.00222958 0.00222203\n",
      " 0.00220312 0.00219438 0.00218776 0.0021722  0.00215608 0.00215051\n",
      " 0.00213564 0.00212784 0.00211271 0.00208695 0.00207613 0.00206077\n",
      " 0.00204613 0.00203201 0.00200931 0.00199865 0.00199714 0.00197327\n",
      " 0.00196589 0.00194951 0.00194085 0.00193257 0.00190954 0.00190278\n",
      " 0.00187657 0.0018573  0.00185606 0.00183413]\n",
      "0.3412776971149227\n",
      "[26.28071313  3.92896916  3.51203674  3.10301032  2.61748589  2.59642773\n",
      "  2.50217867  2.32098855  2.26439245  2.19203943  2.13375967  2.12626195\n",
      "  2.0574806   2.02899959  1.99946031  1.97402069  1.96747448  1.95357853\n",
      "  1.93008885  1.91932804  1.89298024  1.87190046  1.86671996  1.84849394\n",
      "  1.82766938  1.819579    1.80027893  1.77431404  1.74951541  1.7391934\n",
      "  1.72195946  1.71670307  1.70696488  1.69474953  1.67750176  1.67234783\n",
      "  1.66697967  1.65665295  1.64652842  1.64316809  1.63696717  1.62944089\n",
      "  1.61771724  1.61004634  1.59668252  1.59144373  1.58090069  1.57474131\n",
      "  1.57139547  1.56660944  1.54422075  1.54187887  1.53957662  1.53645066\n",
      "  1.52294988  1.51981377  1.5133942   1.51083776  1.50699561  1.50498852\n",
      "  1.4936319   1.48487743  1.47627146  1.47043492  1.4695741   1.461563\n",
      "  1.45597621  1.45119181  1.4500094   1.43903939  1.43480772  1.43237878\n",
      "  1.42628258  1.42346676  1.4212888   1.41623469  1.41104021  1.40916811\n",
      "  1.4042597   1.40182779  1.39671277  1.38815761  1.38456007  1.37944892\n",
      "  1.37451998  1.36984485  1.36221159  1.35849975  1.35798477  1.34983287\n",
      "  1.34729707  1.34169375  1.33869015  1.3358442   1.32784891  1.32550655\n",
      "  1.31632998  1.30959701  1.3091189   1.30136956]\n",
      "(100, 2000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42) #압축할 component의 수 지정\n",
    "svd.fit(X_train_tfidf)  # train data로 학습 -> unsupervised이므로 y가 필요 없음\n",
    "print(svd.explained_variance_ratio_)  #계산된 각 component가 설명하는 분산의 비율\n",
    "print(svd.explained_variance_ratio_.sum())  #선택된 component들이 설명하는 분산의 합 -> 선택한 component의 수에 따라 달라짐\n",
    "# 현재 결과에서는 100개의 선택된 component가 전체 분산의 34% 정도를 설명하고 있음\n",
    "print(svd.singular_values_) #위 그림에서 오른쪽 가운데 대각행렬의 값\n",
    "print(svd.components_.shape) # 위 그림에서 가장 오른쪽 행렬의 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4748993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 2000)\n",
      "(1600, 100)\n"
     ]
    }
   ],
   "source": [
    "X_train_svd = svd.transform(X_train_tfidf) #선택된 component를 이용하여 2,000개의 feature로부터 feature extract (dimension reduce)\n",
    "print(X_train_tfidf.shape) #축소하기 전의 차원\n",
    "print(X_train_svd.shape) #축소된 후의 차원, 2000 -> 100개로 줄어 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe32ff07",
   "metadata": {},
   "source": [
    "### LSA를 이용한 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e069693f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.838\n",
      "Test set score: 0.790\n"
     ]
    }
   ],
   "source": [
    "X_test_svd = svd.transform(X_test_tfidf)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "SVD_clf = LogisticRegression()\n",
    "SVD_clf.fit(X_train_svd, y_train) #축소된 값으로 학습\n",
    "print('Train set score: {:.3f}'.format(SVD_clf.score(X_train_svd, y_train)))\n",
    "print('Test set score: {:.3f}'.format(SVD_clf.score(X_test_svd, y_test)))\n",
    "# NB, Lasso보다 좋지만 Ridge, 일반보다는 나쁜 값\n",
    "# 상황에 따라 달라질 수 있음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
